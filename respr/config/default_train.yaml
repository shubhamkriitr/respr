description: "This is a sample config to run the pipeline"

pipeline:
  name: "TrainingPipelineSimCLR"
instructions:
  do_only_test: false # make sure to set `ckpt_path`
  ckpt_path:
    3: ../../artifacts/2022-11-24_150935/fold_00/lightning_logs/version_0/checkpoints/model-epoch=00-s-step=1-val_mae=9.01.ckpt
    4: ../../artifacts/2022-11-24_150935/fold_00/lightning_logs/version_0/checkpoints/model-epoch=00-s-step=2-val_mae=10.30.ckpt
    
training_init_model_ckpt_path: "../../artifacts/2022-12-16_033756/fold_00\
  /lightning_logs/version_0/checkpoints/model-epoch=01-s-step=10-train_\
  contrastive_loss=1.95.ckpt" # loads weights from here before training



trainer: # currently using trainer from pytorch lightning 
  name: null
  args: []
  kwargs:
    max_epochs: 30
    # not use `check_val_every_n_epoch` if using `val_check_interval`
    # check_val_every_n_epoch: 1
    val_check_interval: 1.0 
    # fast_dev_run: true # for debugging only
    # limit_train_batches: 0.01 # for debugging only
    # overfit_batches=0.01 # for debugging only
    enable_progress_bar: true
    overfit_batches: 5 #batches
    # resume_from_checkpoint: null
    # <<< FOR GPU >>>
    # accelerator: gpu
    # devices: 1

callbacks: [
  {type: model_checkpoint, 
  monitor: train_contrastive_loss,
  save_top_k: 4,
  ckpt_filename: null}
]

model_checkpoint:
  monitor: val_rmse
  save_top_k: 4
  ckpt_filename: null

model: # model => actual model + lighting wrapper
  name: "LitResprMCDropoutCNNSimCLR"
  args: []
  kwargs:
    config:
      optimization:
        lr: 1.0e-3
        weight_decay: 1.0e-4
      num_monte_carlo_rollouts: 10
      model_module_class: ResprMCDropoutCNNResnet18SimCLR
      module_config: # config for the actual pytorch model
      # it will be passed as config kwarg like so `model_module(config=...)`
        input_channels: 1
        force_reshape_input: false
      y_normalization: # added in new #TODO: use this entry
        y_mean: 18.8806 # from CAPNOBASE (as was used for older experiments)
        y_std: 9.8441 # from CAPNOBASE (as was used for older experiments)
      mode_schedule: [[[0, 0], "contrastive"], [[1, 2], "contrastive"]]
      batch_size: 4
      model_save_step: 1



dataloading:
  name: "ResprCsvDataLoaderComposer"
  args: []
  kwargs:
    # config:
      # dataset: "BaseResprCsvDataset"
      # dataset_path: "../../artifacts/frozen/dataset_capnobase_w32_s1_artif_cleaned_commit_c4dec47.csv"
      # batch_size: 16
      # num_workers: 0
      # augmentation: null # to be added later #TODO
      # num_folds: 5
      # start_fold: 0
      # val_split: 0.2
      # test_split: 0.2
      # ckpt_path: ../../artifacts/2022-11-17_050120/fold_00/lightning_logs/version_2482287/checkpoints/epoch=7-step=6717.ckpt
    # MINI DATASET FOR DEV
    config:
      dataset: 
        name: "DatasetAndAugmentationWrapper"
        args: []
        kwargs:
          config:
            underlying_dataset:
              name: "BaseResprCsvDataset"
              args: []
              kwargs:
                config: {}
            data_augmentation:
              name: "BaseResprDataAugmentationComposerSimCLR"
              args: []
              kwargs:
                config: {}
      val_dataset: "BaseResprCsvDatasetDuplicateX"
      test_dataset: "BaseResprCsvDatasetDuplicateX"
      dataset_path: "../../artifacts/frozen/mini/dataset_capnobase_w32_s1_4_subjects.csv" #"../../artifacts/frozen/dataset_capnobase_w32_s1_artif_cleaned_commit_c4dec47.csv"
      x_length: 9600 # dimension of input x
        # sample paths
        #"../../artifacts/frozen/2022-12-07_000441/dataset.csv"
      normalize_x: true 
      normalize_mode: local
      batch_size: 4
      augmentation: null # to be added later #TODO
      num_folds: 2
      start_fold: 0
      val_split: 0.25
      test_split: 0.25
      num_workers: 0



# data_adapter:
#   name: "CapnobaseMatDataAdapter"
#   args: []
#   kwargs:
#     config:
#       data_root_dir: "../../Datasets/bidmc-ppg-and-respiration-dataset-1.0.0/bidmc_csv"


# loss weights 0 to 120 @step 2
# weights: [110.97777777777777, 178.35714285714286, 499.4, 18.916666666666668, 1.288109362909466, 1.0, 4.984031936127744, 2.455260570304818, 4.959285004965244, 4.4629133154602325, 12.12135922330097, 10.275720164609053, 17.34027777777778, 22.597285067873305, 7.815336463223788, 38.713178294573645, 31.2125, 24.600985221674875, 53.69892473118279, 54.879120879120876, 28.375, 86.10344827586206, 499.4, 499.4, 499.4, 499.4, 499.4, 499.4, 499.4, 499.4, 499.4, 499.4, 499.4, 499.4, 499.4, 499.4, 499.4, 499.4, 499.4, 499.4, 499.4, 499.4, 499.4, 499.4, 499.4, 499.4, 499.4, 499.4, 499.4, 499.4, 499.4, 499.4, 499.4, 499.4, 499.4, 499.4, 499.4, 499.4, 499.4, 499.4]



## >>> MODEL 1 >>> 
# model: # model => actual model + lighting wrapper
#   name: "LitResprResnet18LinearScaledMeanHead"
#   args: []
#   kwargs:
#     config:
#       optimization:
#         lr: 1.0e-3
#         weight_decay: 1.0e-4
#       # num_monte_carlo_rollouts: 10
#       # model_module_class: ResprMCDropoutCNNResnet18
#       module_config: # config for the actual pytorch model
#       # it will be passed as config kwarg like so `model_module(config=...)`
#         input_channels: 1
#         force_reshape_input: false
#       y_normalization: # added in new #TODO: use this entry
#         y_mean: 0.
#         y_std: 1.0
#       weighted_loss:
#         do_weighted_loss: false
#         bin_step: 2
#         max_bpm: 120 # this is just for binning and weights
#         loss_weights: [110.97777777777777, 178.35714285714286, 499.4, 18.916666666666668, 1.288109362909466, 1.0, 4.984031936127744, 2.455260570304818, 4.959285004965244, 4.4629133154602325, 12.12135922330097, 10.275720164609053, 17.34027777777778, 22.597285067873305, 7.815336463223788, 38.713178294573645, 31.2125, 24.600985221674875, 53.69892473118279, 54.879120879120876, 28.375, 86.10344827586206, 499.4, 499.4, 499.4, 499.4, 499.4, 499.4, 499.4, 499.4, 499.4, 499.4, 499.4, 499.4, 499.4, 499.4, 499.4, 499.4, 499.4, 499.4, 499.4, 499.4, 499.4, 499.4, 499.4, 499.4, 499.4, 499.4, 499.4, 499.4, 499.4, 499.4, 499.4, 499.4, 499.4, 499.4, 499.4, 499.4, 499.4, 499.4]
#         weight_scale_down_by: 1.0
## <<< MODEL 1 <<<