description: "Leave one out CV
  same as exp 12 + @changes:
    Use class weighting (Code@ cc8b51e517191c)
  same as exp 12e + @changes:
    scale down class weights: by 100"
  
output_dir: "../../artifacts/exp_12f_cnn_on_mean_gt_old_csv_w32_stride4_LOOCV"
pipeline:
  name: "TrainingPipeline"
instructions:
  do_only_test: false # make sure to set `ckpt_path`

trainer: # currently using trainer from pytorch lightning 
  name: null
  args: []
  kwargs:
    max_epochs: 25
    # not use `check_val_every_n_epoch` if using `val_check_interval`
    # check_val_every_n_epoch: 1
    val_check_interval: 0.25 
    # fast_dev_run: true # for debugging only
    # limit_train_batches: 0.01 # for debugging only
    # overfit_batches=0.01 # for debugging only
    enable_progress_bar: true
    # overfit_batches: 5 #batches
    # resume_from_checkpoint: null
    # <<< FOR GPU >>>
    accelerator: gpu
    devices: [2]


model:
  name: "LitResprResnet18"
  args: []
  kwargs:
    config:
      optimization:
        lr: 1.0e-3
        weight_decay: 1.0e-4
      # num_monte_carlo_rollouts: 10
      # model_module_class: ResprMCDropoutCNNResnet18
      module_config: # config for the actual pytorch model
      # it will be passed as config kwarg like so `model_module(config=...)`
        input_channels: 1
        force_reshape_input: false
      y_normalization: # added in new #TODO: use this entry
        y_mean: 0.
        y_std: 1.0
      weighted_loss:
        do_weighted_loss: true
        bin_step: 2
        max_bpm: 120 # this is just for binning and weights
        loss_weights: [110.97777777777777, 178.35714285714286, 499.4, 18.916666666666668, 1.288109362909466, 1.0, 4.984031936127744, 2.455260570304818, 4.959285004965244, 4.4629133154602325, 12.12135922330097, 10.275720164609053, 17.34027777777778, 22.597285067873305, 7.815336463223788, 38.713178294573645, 31.2125, 24.600985221674875, 53.69892473118279, 54.879120879120876, 28.375, 86.10344827586206, 499.4, 499.4, 499.4, 499.4, 499.4, 499.4, 499.4, 499.4, 499.4, 499.4, 499.4, 499.4, 499.4, 499.4, 499.4, 499.4, 499.4, 499.4, 499.4, 499.4, 499.4, 499.4, 499.4, 499.4, 499.4, 499.4, 499.4, 499.4, 499.4, 499.4, 499.4, 499.4, 499.4, 499.4, 499.4, 499.4, 499.4, 499.4]
        weight_scale_down_by: 100.0
dataloading:
  name: "ResprCsvDataLoaderComposer"
  args: []
  kwargs:
    # config:
      # dataset: "BaseResprCsvDataset"
      # dataset_path: "../../artifacts/frozen/dataset_capnobase_w32_s1_artif_cleaned_commit_c4dec47.csv"
      # batch_size: 16
      # num_workers: 0
      # augmentation: null # to be added later #TODO
      # num_folds: 5
      # start_fold: 0
      # val_split: 0.2
      # test_split: 0.2
      # ckpt_path: ../../artifacts/2022-11-17_050120/fold_00/lightning_logs/version_2482287/checkpoints/epoch=7-step=6717.ckpt
    # MINI DATASET FOR DEV
    config:
      dataset: "BaseResprCsvDataset"
      dataset_path: "../../artifacts/frozen/dataset_capnobase_w32_s4_artif_cleaned_gt_mean_commit-f69eea50.csv"
      batch_size: 16
      augmentation: null # to be added later #TODO
      num_folds: 22
      start_fold: 0
      val_split: 0.2
      test_split: 0.023 # ~1/42
      num_workers: 0



# data_adapter:
#   name: "CapnobaseMatDataAdapter"
#   args: []
#   kwargs:
#     config:
#       data_root_dir: "../../Datasets/bidmc-ppg-and-respiration-dataset-1.0.0/bidmc_csv"


# dataset_capnobase_w32_s16_artif_cleaned_gt_mean_commit_cbcf0ac2b00.csv
# dataset_capnobase_w32_s16_artif_cleaned_gt_mid_commit_71f269d1a7.csv
# dataset_capnobase_w32_s1_artif_cleaned_commit_c4dec47.csv
# dataset_capnobase_w32_s1_artif_cleaned_gt_mid_commit_1995ef9cc.csv
# dataset_capnobase_w32_s32_artif_cleaned_gt_mean_commit_176d24c06a.csv
# dataset_capnobase_w32_s32_artif_cleaned_gt_mid_commit_4f6bc5f69fb.csv
# dataset_capnobase_w32_s4_artif_cleaned_gt_mean_commit-f69eea50.csv
# dataset_capnobase_w32_s4_artif_cleaned_gt_mid_commit_63b398dd.csv
# dataset_capnobase_w32_s8_artif_cleaned_gt_mean_commit_0ff4049a.csv
# dataset_capnobase_w32_s8_artif_cleaned_gt_mid_commit_cba0919a699.csv